{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#import and store dataset\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "#import and store dataset\n",
    "\n",
    "credit_card_data= pd.read_csv('creditcard.csv')\n",
    "credit_card_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Steps for the Analysis\n",
    "1. **Shuffle the Data**  \n",
    "   Randomly shuffle the dataset to remove any potential order bias and ensure a fair distribution of classes during splitting.\n",
    "\n",
    "2. **One-Hot Encoding**  \n",
    "   Transform categorical variables (e.g., the `Class` column) into binary columns to make the data suitable for machine learning algorithms.\n",
    "\n",
    "3. **Normalize the Data**  \n",
    "   Scale the feature values to fall within the range [0, 1], ensuring that all features contribute equally to the model.\n",
    "\n",
    "4. **Split Features and Labels (X and y)**  \n",
    "   Separate the dataset into independent variables (X) and target variables (y) for model training.\n",
    "\n",
    "5. **Convert to NumPy Arrays**  \n",
    "   Convert the dataframes to NumPy arrays for efficient numerical computation and compatibility with machine learning libraries.\n",
    "\n",
    "6. **Split Data into Training and Testing Sets**  \n",
    "   Divide the data into training and testing subsets to evaluate the model's performance on unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle and randomize data \n",
    "shuffled_data= credit_card_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding by changing class column into class_0(1,0) for legit and class_1(0,1) for fraudulent data\n",
    "one_hote_data= pd.get_dummies(shuffled_data, columns=['Class'])\n",
    "one_hote_data = one_hote_data.astype(float)\n",
    "normalized_data= (one_hote_data-one_hote_data.min())/(one_hote_data.max()-one_hote_data.min())\n",
    "\n",
    "#store clumns v1 to v28 for df_X and column Class_0 and Class_1 for df_ y\n",
    "df_X= normalized_data.drop(['Class_0', 'Class_1'], axis=1)\n",
    "df_y= normalized_data[[\"Class_0\", \"Class_1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert both dataset into np arrays of float32\n",
    "ar_X, ar_y= np.asarray(df_X.values, dtype=\"float32\"), np.asarray(df_y.values, dtype=\"float32\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Important Note\n",
    "The dataset is highly imbalanced, with significantly fewer instances of fraudulent transactions compared to legitimate ones. To address this imbalance, we can apply **logit weighting**. This technique adjusts the importance of the minority class, ensuring the model pays more attention to underrepresented data during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into 80% traininga an 20% test\n",
    "train_size= int(0.8*len(ar_X))\n",
    "(raw_X_train, raw_y_tarin)= (ar_X[:train_size], ar_y[:train_size])\n",
    "(raw_X_test, raw_y_test)= (ar_X[train_size:], ar_y[train_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let first see the ratio . Note that 0.0017% of the transactions are fraudulent\n",
    "count_legit, count_fraud=  np.unique(credit_card_data['Class'],return_counts=True)[1]\n",
    "fraud_ratio= count_fraud/(count_legit+count_fraud)\n",
    "print('%/ fraud ratio', fraud_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighting= 1/fraud_ratio #this will be multiplied by our y data\n",
    "raw_y_tarin[:,1]= raw_y_tarin[:,1]*weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_y_tarin[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Building the computation model\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 cells for the input\n",
    "input_dimensions= ar_X.shape[1]\n",
    "print(input_dimensions)\n",
    "#2 cells for the output\n",
    "output_dimensions= ar_y.shape[1]\n",
    "print(output_dimensions)\n",
    "# 100 cells for the first layer\n",
    "num_layer_1_cells= 100\n",
    "# 150 cells for the second layer\n",
    "num_layer_2_cells= 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will be used as input to the model when it comes time to train it, we will assing values at run time\n",
    "X_train_node= tf.placeholder(tf.float32, [None, input_dimensions], name=\"X_train\" )\n",
    "y_train_node= tf.placeholder(tf.float32, [None, output_dimensions], name=\"y_train\")\n",
    "\n",
    "#This will be used as inputs to the model once it is time to test the model\n",
    "X_test_node= tf.constant(raw_X_test, name=\"X_test\")\n",
    "y_test_node= tf.constant(raw_y_test, name=\"y_test\")\n",
    "\n",
    "#the first layer takes in input and passes output to 2nd layer\n",
    "weight_1_node= tf.Variable(tf.zeros([input_dimensions,num_layer_1_cells]), name=\"weight_1\")\n",
    "biases_1_node= tf.Variable(tf.zeros([num_layer_1_cells]), name=\"biases_1\")\n",
    "\n",
    "#the second layer takes in input from the first layer and passes output to 3rd layer\n",
    "weight_2_node= tf.Variable(tf.zeros([num_layer_1_cells,num_layer_2_cells]), name=\"weight_2\")\n",
    "biases_2_node= tf.Variable(tf.zeros([num_layer_2_cells]), name=\"biases_2\")\n",
    "\n",
    "#the third layer takes in input from 2nd layer and output [1,0] or [0,1] depending on the case, whether fraud or legit\n",
    "weight_3_node= tf.Variable(tf.zeros([num_layer_2_cells,output_dimensions]), name=\"weight_3\")\n",
    "biases_3_node= tf.Variable(tf.zeros([output_dimensions]), name=\"biases_3\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create a Neural Network Function\n",
    "Develop a function that processes an input tensor through three distinct layers and outputs a tensor indicating whether a transaction is fraudulent or legitimate. Each layer employs a unique activation function to model the relationships within the data and make accurate predictions based on the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network (input_tensor):\n",
    "    layer1= tf.nn.sigmoid(tf.matmul(input_tensor,weight_1_node)+biases_1_node)\n",
    "    #use drop out function to prevent the model from being lazy\n",
    "    layer2= tf.nn.dropout(tf.nn.sigmoid(tf.matmul(layer1,weight_2_node)+biases_2_node),0.85)\n",
    "    #use sofmax function because it works well with one-hot coding\n",
    "    layer3= tf.nn.softmax(tf.matmul(layer2,weight_3_node)+biases_3_node)\n",
    "    return layer3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create a Prediction Function\n",
    "This function is designed to predict outcomes based on the input training or testing data. It's important to note that `x_train_node` serves as a placeholder, with actual values being provided dynamically at runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_prediction= network(X_train_node)\n",
    "print(y_train_prediction)\n",
    "y_test_prediction= network(X_test_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss function measures difference between actual output and predicted output\n",
    "cross_entropy= tf.losses.softmax_cross_entropy(y_train_node, y_train_prediction)\n",
    "\n",
    "#The adam optimizer function will try to minimize loss(cross_entropty) but changing 3 layers' variable\n",
    "#values at a learning rate of 0.005\n",
    "optimizer= tf.train.AdamOptimizer(0.005).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to calulate the accuracy\n",
    "def calculate_accuracy(actual,predicted):\n",
    "    actual= np.argmax(actual,1)\n",
    "    predicted= np.argmax(predicted,1)\n",
    "    return (100*np.sum(np.equal(predicted,actual))/predicted.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs= 100\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time= time.time()\n",
    "        _,cross_entropy_score= session.run([optimizer,cross_entropy], \n",
    "                                           feed_dict= {X_train_node:raw_X_train,\n",
    "                                                       y_train_node:raw_y_tarin})\n",
    "        if epoch %10 ==0:\n",
    "            timer= time.time()- start_time\n",
    "\n",
    "            print('Epoch: {}'.format(epoch), 'Cureent loss: {0:.5f}'.format(cross_entropy_score),\n",
    "                   'Elapsed time: {0:.2f}'.format(timer))\n",
    "            \n",
    "            final_y_test= y_test_node.eval()\n",
    "            final_y_test_prediction= y_test_prediction.eval()\n",
    "            final_accuracy= calculate_accuracy(final_y_test, final_y_test_prediction)\n",
    "            print('current accuracy {0:.2f}%'.format(final_accuracy))\n",
    "\n",
    "    final_y_test= y_test_node.eval()\n",
    "    final_y_test_prediction= y_test_prediction.eval()\n",
    "    final_accuracy= calculate_accuracy(final_y_test, final_y_test_prediction)\n",
    "    print('final accuracy {0:.2f}%'.format(final_accuracy))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fraud_y_test= final_y_test[final_y_test[:,1]==1]\n",
    "final_fraud_y_test_prediction= final_y_test_prediction[final_y_test[:,1]==1]\n",
    "final_fraud_accuracy= calculate_accuracy(final_fraud_y_test,final_fraud_y_test_prediction)\n",
    "print('Final fraud specific accuracy: {0:.2f}%'.format(final_fraud_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
